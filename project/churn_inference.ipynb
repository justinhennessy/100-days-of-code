{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import random\n",
    "from fastai.imports import *\n",
    "from fastai.structured import *\n",
    "from matplotlib import pyplot\n",
    "from pandas_summary import DataFrameSummary\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from IPython.display import display\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import trim_mean, kurtosis\n",
    "from scipy.stats.mstats import mode, gmean, hmean\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import f1_score,\\\n",
    "    accuracy_score, confusion_matrix,\\\n",
    "    precision_score, recall_score,\\\n",
    "    roc_curve, roc_auc_score,\\\n",
    "    cohen_kappa_score, mean_absolute_error,\\\n",
    "    precision_recall_curve, auc,\\\n",
    "    average_precision_score\n",
    "\n",
    "set_plot_sizes(12,14,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df_raw):\n",
    "    # Sort data by date\n",
    "    df_raw = df_raw.sort_values(by='licence_registration_date')\n",
    "    \n",
    "    # Convert annual_revenue from a string to a float\n",
    "    df_raw['annual_revenue'] = pd.to_numeric(df_raw['annual_revenue'].str.replace(',', ''))\n",
    "    \n",
    "    # Convert fields to INT and setting any NaNs to the mean of that type\n",
    "    case_types = ['cases_total','cases_open','cases_closed','cases_age_hours_total','cases_age_hours_average', 'last_login_days']\n",
    "\n",
    "    for case_type in case_types:\n",
    "        default_value = df_raw[case_type].fillna(df_raw[case_type].median())\n",
    "        df_raw[case_type] = df_raw[case_type].fillna(default_value).astype(int)\n",
    "    \n",
    "    # Fix missing values for annual revenue, replace with mean/trimmed mean of the plan size they are on\n",
    "    plan_list = df_raw.plan[~pd.isnull(df_raw.plan)].unique()\n",
    "\n",
    "    for plan in plan_list:\n",
    "        mean = round(df_raw.annual_revenue[df_raw.plan == plan].mean(), 2)\n",
    "        trimmed_mean = trim_mean(df_raw.annual_revenue[df_raw.plan == plan].values, 0.1)\n",
    "    \n",
    "        if pd.isnull(mean):\n",
    "            revenue = 0\n",
    "        else:\n",
    "            revenue = mean\n",
    "        df_raw.loc[df_raw.plan==plan, 'annual_revenue'] = df_raw.loc[df_raw.plan==plan, 'annual_revenue'].fillna(revenue)\n",
    "        \n",
    "    # 'bin' last login days\n",
    "\n",
    "    bins = [1, 3, 7, 14, 30, 60]\n",
    "    group_names = ['day', 'few_days', 'week', 'fortnight', 'month']\n",
    "\n",
    "    # need to get the mean of the plan size for last_login_days and set each row to that\n",
    "    #df_raw.last_login_days = df_raw.last_login_days.fillna(np.mean(df_raw.last_login_days))\n",
    "\n",
    "    last_login_categories = pd.cut(df_raw['last_login_days'], bins, labels=group_names)\n",
    "    df_raw['last_login_categories'] = pd.cut(df_raw['last_login_days'], bins, labels=group_names)\n",
    "    #pd.value_counts(df_raw['last_login_categories'])\n",
    "    \n",
    "    # one-hot encode fields\n",
    "    dummy_columns = ['customer_account_status', 'last_login_categories', 'plan']\n",
    "\n",
    "    for dummy_column in dummy_columns:\n",
    "        dummy = pd.get_dummies(df_raw[dummy_column], prefix=dummy_column)\n",
    "        df_raw = pd.concat([df_raw,dummy], axis=1)\n",
    "        df_raw = df_raw.drop(columns=dummy_column)\n",
    "        \n",
    "    \n",
    "    # This breaks all the date features up into number columns\n",
    "    # These steps can only be run once then you need to comment them out\n",
    "    add_datepart(df_raw, 'licence_registration_date')\n",
    "    add_datepart(df_raw, 'golive_date')\n",
    "    \n",
    "    # Drop columns, some of these create \"Data Leakage\", some are just to test if it has impact when they are taken out\n",
    "    df_raw = df_raw.drop(columns=['customer_account_status_Good', 'last_login_concern',\n",
    "                                  'last_login_days', 'account_status', 'changing_platform', \n",
    "                                  'new_platform', 'licence_status', 'canceldate', \n",
    "                                  'cancel_details', 'cancel_reason'])\n",
    "    \n",
    "    # Set default values for NaN values in NPS\n",
    "    df_raw.nps = df_raw.nps.fillna(np.nanmean(df_raw.nps))\n",
    "\n",
    "    # Set NaN to zero\n",
    "    features = ['churned', 'interactions_total', 'interactions_completed', 'interactions_no_response', 'interactions_no_onboarding', 'interactions_completed_training']\n",
    "\n",
    "    for feature in features:\n",
    "        df_raw[feature] = df_raw[feature].fillna(0)\n",
    "        \n",
    "    # Complete the transformation of all data into numbers using proc_df and create training dataframes\n",
    "    train_cats(df_raw)\n",
    "    \n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data\n",
    "\n",
    "PATH = \"../data/\"\n",
    "df_raw = pd.read_csv(f'{PATH}churn.csv', low_memory=False, \n",
    "                     parse_dates=['canceldate', 'licence_registration_date', 'golive_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2780 (2780, 38)\n",
      "5889 (5889, 99)\n",
      "1351 (1351, 102)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justinhennessy/anaconda3/envs/fastai/lib/python3.6/site-packages/pandas/core/frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "print(len(df_raw[df_raw.churned == 0]), df_raw[df_raw.churned == 0].shape)\n",
    "df_processed = prepare_data(df_raw)\n",
    "print(len(df_processed), df_processed.shape)\n",
    "df_filtered = df_processed[df_processed.licence_registration_Year >2017]\n",
    "df_data, y_data, nas = proc_df(df_filtered, 'churned')\n",
    "print(len(df_data), df_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2780 353\n"
     ]
    }
   ],
   "source": [
    "joblib_file = \"churn_model.pkl\"\n",
    "\n",
    "# Load from file\n",
    "churn_model = joblib.load(joblib_file)\n",
    "\n",
    "predictions = churn_model.predict(df_data)\n",
    "probability = churn_model.predict_proba(df_data)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "array = []\n",
    "for i in range(len(df_filtered[df_filtered.churned == 0].username)):\n",
    "    if predictions[i] == 1:\n",
    "        array.append([now,df_filtered[df_filtered.churned == 0].username.iloc[i],df_filtered[df_filtered.churned == 0].annual_revenue.iloc[i],predictions[i],probability[i][1]])\n",
    "        \n",
    "\n",
    "print(len(df_raw[df_raw.churned == 0]), len(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "churn_concerns = []\n",
    "\n",
    "for i in range(len(array)):\n",
    "    if array[i][4] > .88:\n",
    "        count += 1\n",
    "        churn_concerns.append(array[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N052273,3161843.85,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N055220,800290.23,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N054211,649310.58,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N052113,485805.76,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N056267,393561.83,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N059921,330861.34,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N055984,295827.07,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N053827,257118.25,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N060326,87983.24,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N047717,73909.26,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N060976,70305.75,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N056363,51103.36,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N061812,49879.88,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N062111,49879.88,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N060521,48159.26,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N056088,44007.72,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N060421,43651.09,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N056151,31767.57,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N056115,29618.5,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N055639,25441.19,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N054480,22360.39,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N061232,21143.91,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N058214,11489.68,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N061685,10782.22,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N058482,5397.89,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N062334,4451.42,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N054783,4371.81,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N056294,1762.37,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N058089,762.54,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N057941,596.98,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N060207,522.58,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N052835,392.73,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N056847,257.26,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N039257,192.64,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N061188,130.79,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N058014,120.79,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N062219,48.1,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N052316,45.64,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N053422,0.0,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N054043,0.0,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N055062,0.0,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N055343,0.0,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N056863,0.0,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N058854,0.0,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N061112,0.0,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N060830,0.0,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N061491,0.0,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N063244,0.0,1\n",
      "2019-10-05 13:55:33.754868,2019-10-05 13:51:39.613636,N064187,0.0,1\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "import csv\n",
    "\n",
    "sorted_array = sorted(churn_concerns, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "for i in range(len(sorted_array)):\n",
    "    print(f\"{now},{sorted_array[i][0]},{sorted_array[i][1]},{sorted_array[i][2]},{sorted_array[i][3]}\")\n",
    "    with open('../data/predictions.csv', 'w') as writeFile:\n",
    "        writer = csv.writer(writeFile)\n",
    "        writer.writerows(sorted_array)\n",
    "\n",
    "writeFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/1292bacee4b6c087ed3ec76def23c344"
  },
  "gist": {
   "data": {
    "description": "courses/ml1/churn.ipynb",
    "public": false
   },
   "id": "1292bacee4b6c087ed3ec76def23c344"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
