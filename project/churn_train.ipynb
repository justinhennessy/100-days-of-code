{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages needed\n",
    "\n",
    "#!pip install \"tabulate\" \"pandas_summary\" \"imblearn\" \"fastai\" \"joblib\"\n",
    "#!pip install fastai==0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import random\n",
    "from fastai.imports import *\n",
    "from fastai.structured import *\n",
    "from matplotlib import pyplot\n",
    "from pandas_summary import DataFrameSummary\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from IPython.display import display\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import trim_mean, kurtosis\n",
    "from scipy.stats.mstats import mode, gmean, hmean\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import f1_score,\\\n",
    "    accuracy_score, confusion_matrix,\\\n",
    "    precision_score, recall_score,\\\n",
    "    roc_curve, roc_auc_score,\\\n",
    "    cohen_kappa_score, mean_absolute_error,\\\n",
    "    precision_recall_curve, auc,\\\n",
    "    average_precision_score\n",
    "\n",
    "set_plot_sizes(12,14,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "PROBABILITY_CUTOFF = 0.50\n",
    "\n",
    "# Utility functions\n",
    "def plot_roc_pr(m, X_valid, y_valid):\n",
    "    # Generate the probabilities\n",
    "    #y_pred_prob = generate_predictions(X_valid)\n",
    "    y_pred_prob = m.predict_proba(X_valid)\n",
    "    y_pred_prob = y_pred_prob[:, 1]\n",
    "\n",
    "    # Calculate the roc metrics\n",
    "    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_prob)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    plt.plot(fpr,tpr, label='ROC')\n",
    "\n",
    "    # Add labels and diagonal line\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label='no skill')\n",
    "\n",
    "    # Plot a precision-recall curve\n",
    "    precision, recall, thresholds = precision_recall_curve(y_valid, y_pred_prob)\n",
    "    area_under_curve = auc(recall, precision)\n",
    "    ap = average_precision_score(y_valid, y_pred_prob)\n",
    "\n",
    "    # plot no skill\n",
    "    pyplot.plot([0, 1], [0.5, 0.5], linestyle='--', label='no skill')\n",
    "    # plot the precision-recall curve for the model\n",
    "    pyplot.plot(recall, precision, marker='.', label='precision-recall')\n",
    "\n",
    "    legend = ax.legend(loc='best', shadow=True, fontsize='medium')\n",
    "\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "    \n",
    "    # Output AUC and average precision score\n",
    "    print('auc=%.3f ap=%.3f' % (area_under_curve, ap))\n",
    "    \n",
    "def uber_score(y_valid, validate_predictions):\n",
    "    return [precision_score(y_valid,validate_predictions), recall_score(y_valid,validate_predictions), f1_score(y_valid,validate_predictions), accuracy_score(y_valid,validate_predictions), cohen_kappa_score(y_valid,validate_predictions), mean_absolute_error(y_valid,validate_predictions)]\n",
    "\n",
    "def graph_corr(df):\n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    corr = df.corr()\n",
    "    sns.heatmap(corr, cmap='YlGnBu', annot_kws={'size':30}, ax=ax)\n",
    "    ax.set_title(\"Correlation Matrix\", fontsize=14)\n",
    "    plt.show()\n",
    "    \n",
    "def data_summary(df_raw):\n",
    "    array = []\n",
    "    for column_name in df_raw.select_dtypes(include=['float64', 'int64']).columns:\n",
    "        mean = df_raw[column_name].mean()\n",
    "        median = df_raw[column_name].median()\n",
    "        std = df_raw[column_name].std()\n",
    "        cv = df_raw[column_name].std()/df_raw[column_name].mean()\n",
    "        trimmed_mean = trim_mean(df_raw[column_name].values, 0.1)\n",
    "        array.append([column_name, mean, median, std, cv, trimmed_mean])\n",
    "    print(tabulate(array,headers=['Column', 'Mean', 'Median', 'Std', 'cv', 'Trimmed Mean']))\n",
    "\n",
    "def generate_predictions(X_valid, cutoff=PROBABILITY_CUTOFF):\n",
    "    return m.predict(X_valid)\n",
    "    #return (m.predict_proba(X_valid)[:,1] >= cutoff).astype(bool)\n",
    "    \n",
    "def conf_matrix(y_valid, validate_predictions):\n",
    "    cm = confusion_matrix(y_valid, validate_predictions)\n",
    "    print(cm)\n",
    "    \n",
    "def remove_columns_test(df):\n",
    "    m = RandomForestClassifier(\n",
    "        n_estimators=10,\n",
    "        min_samples_leaf=1, \n",
    "        max_features='sqrt',\n",
    "        n_jobs=-1, \n",
    "        #oob_score=True,\n",
    "        max_depth=3,\n",
    "        bootstrap=False,\n",
    "        criterion='gini',\n",
    "        class_weight={0: 2, 1: 1})\n",
    "    x, _ = split_vals(df, n_trn)\n",
    "    m.fit(x, y_train)\n",
    "    y_pred = m.predict(x)\n",
    "    return uber_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data\n",
    "\n",
    "PATH = \"../../data/\"\n",
    "df_raw = pd.read_csv(f'{PATH}churn.csv', low_memory=False, \n",
    "                     parse_dates=['canceldate', 'licence_registration_date', 'golive_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df_raw):\n",
    "    # Sort data by date\n",
    "    df_raw = df_raw.sort_values(by='licence_registration_date')\n",
    "    \n",
    "    # Convert annual_revenue from a string to a float\n",
    "    df_raw['annual_revenue'] = pd.to_numeric(df_raw['annual_revenue'].str.replace(',', ''))\n",
    "    \n",
    "    # Convert fields to INT and setting any NaNs to the mean of that type\n",
    "    case_types = ['cases_total','cases_open','cases_closed','cases_age_hours_total','cases_age_hours_average', 'last_login_days']\n",
    "\n",
    "    for case_type in case_types:\n",
    "        default_value = df_raw[case_type].fillna(df_raw[case_type].median())\n",
    "        df_raw[case_type] = df_raw[case_type].fillna(default_value).astype(int)\n",
    "    \n",
    "    # Fix missing values for annual revenue, replace with mean/trimmed mean of the plan size they are on\n",
    "    plan_list = df_raw.plan[~pd.isnull(df_raw.plan)].unique()\n",
    "\n",
    "    for plan in plan_list:\n",
    "        mean = round(df_raw.annual_revenue[df_raw.plan == plan].mean(), 2)\n",
    "        trimmed_mean = trim_mean(df_raw.annual_revenue[df_raw.plan == plan].values, 0.1)\n",
    "    \n",
    "        if pd.isnull(mean):\n",
    "            revenue = 0\n",
    "        else:\n",
    "            revenue = mean\n",
    "        df_raw.loc[df_raw.plan==plan, 'annual_revenue'] = df_raw.loc[df_raw.plan==plan, 'annual_revenue'].fillna(revenue)\n",
    "        \n",
    "    # 'bin' last login days\n",
    "\n",
    "    bins = [1, 3, 7, 14, 30, 60]\n",
    "    group_names = ['day', 'few_days', 'week', 'fortnight', 'month']\n",
    "\n",
    "    # need to get the mean of the plan size for last_login_days and set each row to that\n",
    "    #df_raw.last_login_days = df_raw.last_login_days.fillna(np.mean(df_raw.last_login_days))\n",
    "\n",
    "    last_login_categories = pd.cut(df_raw['last_login_days'], bins, labels=group_names)\n",
    "    df_raw['last_login_categories'] = pd.cut(df_raw['last_login_days'], bins, labels=group_names)\n",
    "    #pd.value_counts(df_raw['last_login_categories'])\n",
    "    \n",
    "    # one-hot encode fields\n",
    "    dummy_columns = ['customer_account_status', 'last_login_categories', 'plan']\n",
    "\n",
    "    for dummy_column in dummy_columns:\n",
    "        dummy = pd.get_dummies(df_raw[dummy_column], prefix=dummy_column)\n",
    "        df_raw = pd.concat([df_raw,dummy], axis=1)\n",
    "        df_raw = df_raw.drop(columns=dummy_column)\n",
    "        \n",
    "    \n",
    "    # This breaks all the date features up into number columns\n",
    "    # These steps can only be run once then you need to comment them out\n",
    "    add_datepart(df_raw, 'licence_registration_date')\n",
    "    add_datepart(df_raw, 'golive_date')\n",
    "    \n",
    "    # Drop columns, some of these create \"Data Leakage\", some are just to test if it has impact when they are taken out\n",
    "    df_raw = df_raw.drop(columns=['customer_account_status_Good', 'last_login_concern',\n",
    "                                  'last_login_days', 'account_status', 'changing_platform', \n",
    "                                  'new_platform', 'licence_status', 'canceldate', \n",
    "                                  'cancel_details', 'cancel_reason'])\n",
    "    \n",
    "    # Set default values for NaN values in NPS\n",
    "    df_raw.nps = df_raw.nps.fillna(np.nanmean(df_raw.nps))\n",
    "\n",
    "    # Set NaN to zero\n",
    "    features = ['churned', 'interactions_total', 'interactions_completed', 'interactions_no_response', 'interactions_no_onboarding', 'interactions_completed_training']\n",
    "\n",
    "    for feature in features:\n",
    "        df_raw[feature] = df_raw[feature].fillna(0)\n",
    "        \n",
    "    # Complete the transformation of all data into numbers using proc_df and create training dataframes\n",
    "    train_cats(df_raw)\n",
    "    \n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5889 (5889, 38)\n",
      "5889 (5889, 99)\n"
     ]
    }
   ],
   "source": [
    "print(len(df_raw), df_raw.shape)\n",
    "df_processed = prepare_data(df_raw)\n",
    "print(len(df_processed), df_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5889 (5889, 103)\n"
     ]
    }
   ],
   "source": [
    "df_trn, y_trn, nas = proc_df(df_processed, 'churned')\n",
    "print(len(df_trn), df_trn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vals(a,n): return a[:n], a[n:]\n",
    "\n",
    "n_valid = 500\n",
    "n_trn = len(df_trn)-n_valid\n",
    "X_train, X_valid = split_vals(df_trn, n_trn)\n",
    "y_train, y_valid = split_vals(y_trn, n_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6056, 103)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fix the dataset imbalance\n",
    "\n",
    "sm = SMOTE(random_state=12, ratio = 1.0)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "X_train_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['churn_model.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    min_samples_leaf=2, \n",
    "    max_features='sqrt',\n",
    "    n_jobs=-1, \n",
    "    #oob_score=True,\n",
    "    max_depth=3,\n",
    "    bootstrap=False,\n",
    "    criterion='gini',\n",
    "    #class_weight={0: 2, 1: 1}\n",
    "    )\n",
    "\n",
    "m.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Save to file in the current working directory\n",
    "joblib_file = \"churn_model.pkl\"\n",
    "joblib.dump(m, joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/1292bacee4b6c087ed3ec76def23c344"
  },
  "gist": {
   "data": {
    "description": "courses/ml1/churn.ipynb",
    "public": false
   },
   "id": "1292bacee4b6c087ed3ec76def23c344"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
